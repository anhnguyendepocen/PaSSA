---
title: "Power and Sample Size Analysis"
author: "Clay Ford"
date: Spring 2016
output: beamer_presentation
---

## Topics

- Quick intro to power and sample size concepts
- Power and sample size for statistical tests
- Sample size for parameter estimation within a given margin of error


## What is power?

Power is the probability a statistical test will detect a hypothesized effect.

In a hypothesis test, one of two things true:

1. Null Hypothesis: No effect
2. Alternative Hypothesis: _some_ effect

We would like to design an experiment such we have a high probability (or high power) of rejecting #1 if #2 is true. The usual desired power is 0.80, or 80%.

## Working Toy Example

I suspect most people place name tags on the left side of their chest (probably because most people are right handed). I create an experiment to test this hunch. I randomly sample $n$ people and count the number of people, $X$, who place a name tag on the left. I want my experiment to have a high probability (power) of verifying this hunch if it's true.

## Calculating power

To calculate power for my name tag experiment, I have to set certain values in advance:

- the number of randomly selected people (_sample size_)
- the probability of placing tag on left assuming my hypothesis is correct (_effect size_)
- the number of tags on left that convince me the hypothesis is true (_critical region_), or a p-value cutoff that convinces me my hypothesis true (_significance level_)

I also have to assume a known distribution for total number of name tags placed on left. In this case, a binomial distribution.

We see that "calculating power" is really just a thought experiment. 


## One possible experiment

Let $X$ equal number of tags placed on left. Conclude people prefer left if I observe $X > 14$ "lefts" in 20 trials. Assume people prefer left 60% of the time and that $X$ has binomial distribution.

Power is $P(X > 14 | n = 20, p = 0.60)$

To calculate in R:
```{r}
pbinom(q = 14, size = 20, prob = 0.6, lower.tail = F)
```

Not very powerful assuming my estimate of 60% left preference is correct.

## Another possible experiment

Let's increase sample size.

Conclude people prefer left if I observe $X > 59$ "lefts" in 100 trials. Assume people prefer left 60% of the time.

Power is $P(X > 59 | n = 100, p = 0.60)$.

```{r}
pbinom(q = 59, size = 100, prob = 0.6, lower.tail = F)
```

Better, but still not close to 0.80. We see increasing sample size increased power.

## Yet another possible experiment

Conclude people prefer left if I observe $X > 59$ "lefts" in 100 trials. Assume people prefer left 65% of the time.

Power is $P(X > 59 | n = 100, p = 0.65)$.

```{r}
pbinom(q = 59, size = 100, prob = 0.65, lower.tail = F)
```

We achieved a high power, assuming we're correct about the probability of left-placement being 65%. We see simply increasing the assumed likelihood of "left" (or increasing the _effect size_) increased power.

## Type II error

What if conclude people have no preference even if they truly prefer left 65% of the time? I have made a _Type II error_.

Type II error, or $\beta$,  is $P(X \le 59 | n = 100, p = 0.65)$.

```{r}
pbinom(q = 59, size = 100, prob = 0.65)
```

Type II error is just 1 $-$ Power. Likewise, Power is $1 - \beta$.

## Type I error

What if I conclude people prefer left when they actually do not? I have made a _Type I error_. 

Type I error, or $\alpha$, is $P(X > 59 | n = 100, p = 0.5)$.

```{r}
pbinom(q = 59, size = 100, prob = 0.5, lower.tail = F)
```

We usually want Type I error, $\alpha$, to be less than 0.05. In practice it is set to a  fixed value, called the _significance level_, and used in formulas to calculate power.   

## Determining sample size

We usually want to determine a sample size that gives us a desired power. 

How many people do I need in my experiment to verify people prefer left 65% of the time with 80% power?

More is better, but "more" can mean unnecessary time and money. Why recruit 100 people if 80 will do? 

## Estimating sample size 

Assume people prefer left 65% of the time and we reject the notion that there is no preference if we witness 60% or more "lefts" at the end of our experiment.

Let's see how four different sample sizes affect power:

```{r}
n <- c(70, 80, 90, 100)
pbinom(q = 0.60*n, size = n, prob = 0.65, lower.tail = F) 
```

It appears 80 people provide sufficient power (about 80%).

## Should we recruit 80 people instead of 100?

What happens to Type I error? 60% of 80 = 48. We reject null if we see 48 or more people with tags on left.

Type I error is $P(X > 47 | n = 80, p = 0.5)$.

```{r}
pbinom(q = 47, size = 80, prob = 0.5, lower.tail = F)
```

We now have a little higher chance of incorrectly concluding people have no preference given they truly do have a preference.

## Visualizing a hypothesis test

```{r echo=FALSE}
p1 <- dbinom(x = 0:100, size = 100, prob = 0.65)
p2 <- dbinom(x = 0:100, size = 100, prob = 0.5)
op <- par(mfrow=c(2,1))
plot(0:100,p2, type="h", main="Null: No preference (50% chance), n = 100", ylab="P", xlab="Number of name tags on left")
abline(v = 60, col="red")
text(80,0.05, "Incorrect Decision (Type I error)")
text(35,0.05, "Correct Decision")
plot(0:100,p1, type="h", main="Alt: Prefer left (65% chance), n = 100", ylab="P", xlab="Number of name tags on left")
abline(v = 60, col="red")
text(85,0.05, "Correct Decision (Power)")
text(40,0.05, "Incorrect Decision (Type II error)")
par(op)
```


## Calculating power and sample size in practice

In the preceding toy example we "manually" calculated power while tweaking sample size and effect size. In practice we use software to do this for us. 

Power and sample size formulas have been derived for many statistical tests that allows us to...

- calculate **sample size** given power, effect size and significance level
- calculate **power** given sample size, effect size and significance level

The parameters in the formulas are related such that one is determined by the other three. 

## The `pwr` package

Today we'll use R and the `pwr` package.

`install.packages("pwr")`   
`library(pwr)`

The `pwr` package implements power and sample size analyses as described in _Statistical Power Analysis for the Behavioral Sciences (2nd ed.)_, Cohen (1988).

One of the tricks to using the `pwr` package is understanding how it defines _effect size_.

## Effect size

Cohen defines "effect size" as "the degree to which the null hypothesis is false."

Example: If our null mean is 100m, and the alternative is 120m, the effect size is 20m.

But the functions in the `pwr` package require the effect size to be metric-free (unitless). 

**This means you need to calculate effect size before using `pwr` functions. Entering the wrong effect size leads to incorrect power and sample size estimates!**

Fortunately the `pwr` package provides functions for this.

## The `pwr` functions

- `pwr.p.test`: test for one proportion (ES=h)    
- `pwr.2p.test`: test for two proportions (ES=h)     
- `pwr.2p2n.test`: test for two proportions (ES=h, unequal sample sizes)    
- `pwr.t.test`: one sample and two samples (equal sizes) t tests for means (ES=d)     
- `pwr.t2n.test`: two samples (different sizes) t test for means (ES=d)    
- `pwr.chisq.test`: chi-squared test (ES=w)
- `pwr.r.test`: correlation test (ES=r)     
- `pwr.anova.test`: test for one-way balanced anova (ES=f)    
- `pwr.f2.test`: test for the general linear model (ES=f2)     

Notice the various effect sizes: h, d, w, r, f and f2. These are based on formulas.

## The `ES` functions

- `ES.h`: compute effect size h for proportions tests
- `ES.w1`: compute effect size w1 for chi-squared test for goodness of fit
- `ES.w2`: compute effect size w2 for chi-squared test for association
- `cohen.ES`: return conventional effect size (small, medium, large) for all tests available in `pwr`

We will use these functions as needed in the examples that follow.

Other effect sizes - d, r, f, and f2 - must be calculated by hand.
 

## Conventional effect size

Sometimes we don't know the precise effect size we expect or hope to find. In this case we can resort to conventional effect sizes of "small", "medium", or "large".

The `cohen.ES` function generates these for us according to the statistical test of interest.

For example, a "medium" effect size for a proportion test:
`cohen.ES(test="p", size="medium")`

This returns 0.5.

Let's go to R!



## Estimating a population parameter

- Some populations are too big or too difficult to completely measure.
- This means we have to estimate population parameters such as a mean ($\mu$) or a proportion ($p$).
- We estimate a parameter by randomly sampling a subset of the population and calculating a statistic, such as $\bar{x}$ or $\hat{p}$.
- The precision of our estimate is determined by our sample size, $n$, and the variability of the population, $\sigma$. (Notice that $\sigma$ is also a population parameter!)

## The confidence interval

- When estimating a parameter, it is good practice to provide a confidence interval that gives an indication of the uncertainty in our estimtate.
- The usual form of a confidence interval is
$$estimate \pm margin \ of \ error$$
where the margin of error formula depends on the parameter we're estimating.

## The margin of error

- The margin of error is usually just some multiple of a _standard error_.
- Standard error is determined in part by your sample size, $n$.
- For example, if our population is normally distributed with standard deviation $\sigma$, our estimate of the population mean, $\bar{x}$, has standard error $\sigma / \sqrt{n}$ 
 
## Sample size for precision

- If we're willing to estimate the population standard deviation, we can solve for $n$ given a desired margin of error, $\epsilon$.
- Doing this for a mean:
$$\epsilon = z \frac{\sigma}{\sqrt{n}}$$
$$\sqrt{n} = \frac{z \sigma}{\epsilon}$$
$$n = \left(\frac{z \sigma}{\epsilon}\right)^{2}$$
where $z$ is a quantile of a standard normal distribution, such as $1.96$.

## Example

I want to estimate the mean amount of time someone takes a drink from the water fountains in Clark Hall within 0.5 seconds with 95% confidence ($z = 1.96$). I assume the population of drinking times has a standard deviation of 2 seconds.

$$n = \left(\frac{1.96 \times 2}{0.5}\right)^{2}$$

$$n = 62$$

Always round the answer up. Obviously this is approximate. I might treat this as a lower bound. 

## Other sample size estimates

The same kinds of estimates can be made for proportions, difference of proportions, and difference of means, as well as other statistics which have a tractable standard error estimate.

These estimates are implemented in the R package _nandc_. 

Using the previous example:
```{r}
# install.packages("nandc")
library(nandc)
meanSampleSize(moe = 0.5, sd = 2, conf = 0.95)
```

Let's go to R!

## Other R packages of potential interest

- TrialSize. Functions and examples from the book _Sample Size Calculation in Clinical Research_

## References

Cohen, J. (1988). _Statistical Power Analysis for the Behavioral Sciences (2nd ed.)_. LEA.

Hogg, R and Tanis, E. (2006). _Probability and Statistical Inference (7th ed.)_. Pearson.

## Thanks for coming today!

For help and advice with your data analysis, contact us to set up an appointment: statlab@virginia.edu

Sign up for more workshops or see past workshops:
http://data.library.virginia.edu/statlab/

Register for the Research Data Services newsletter to stay up-to-date on StatLab events and resources: http://data.library.virginia.edu/newsletters/
